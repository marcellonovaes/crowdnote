SLIDE 1
Hello everyone, I going to talk about 

How to get complex media annotation from the crowd.

The approach I will present to you is related to my doctoral project
and you can use it for different applications


SLIDE 2
Media annotation is something that usually requires specific resources
like experts and elaborated annotation systems, like SEVino.

The problem is, these resources are not always available.

In certain scenarios,  Deep learning and rule-based systems can generate media annotation 
automatically, but only works in certains conditions.

Such as: 	well delimited context, well-structured media objects, 
			and cases at you have example databases for training deep learning systems.
			
Whan you have wild conditions, chaotic scenarios,
like unplanned videos generated by users, 
...automatic techniques many times don't find the requirements to work well.

In these cases an human computation approach can be a good option, 
because humans can handle chaotic conditions.

SLIDE 3
In a human computation scenarios your tasks are HITs (Human Computation Tasks)

and these tasks can be simple or complex ones.

Simple tasks, use simple annotation tools, in wich workers can execute their jobs with few interactions.
These tasks usually are easy and can be done quickly by unskilled user without specific training.

The problema is, with simple tasks you generate simple results.

in this example.. users had to watch surveillance videos end select the interval 
when some issue happen.. like trespassing, assaults ...etc 


Ok, but sometimes you need more complex annotations, 
is this example workers had to create learning objects,
 
...sometimes you need to annotate different aspects os media objects and relate these annotations..
like in our next experiment involving crowdsourcing mulsemedia annotation. 

In these cases you need a complex annotation tool, 
and your crowd must be composed by trained workers.


SLIDE 4
My objective is generate complex media annotation, using a ordinary crowd.
with unskilled and untrained worker.

For this, all tasks should be Small, Quick, and Simple, 
this kind of task we call, microtask.

Small enough to don't be exhaustive,
Quick enough to worker accept stop and execute it,
and Simple enough to not require training ...only simple instruction.


SLIDE 5
When you collect contribution from the crowd, you can't trust in the workers as individuals.

But you can trust them as a crowd.

It means you can't get the contribution of a worker and use it as your result,
but... you can process the contributions, filter them, apply techniques of counting, similarity,
frequency... use some statistics, and aggregate them in a satisfatory result.

It is based in the Francis Galton work about Wisdom of Crowds. 

In my approach you can use 3 kinds of aggregation:
Automatic, when you can generate a result by applying algorithms over the contributions.

Supervised: when you can apply automatic methods, but you need to use some human moderation
to verify or adequate the results.

an Manual: when you can't specify a method to aggregate the result and you need to do it manually.
This kind of aggregation is suiteble to creative and subjective contributions, for example.



SLIDE 6
In my approach, a propose to analise and divide your complex annotation problem in a set of microtasks,
and use them to compose a process workflow.

You can see that each microtask sub-process is composed of two activities, the task and the aggregation.

This is a cascading microtasks workflow in wich the output of a microtask feeds the next one as input.


SLIDE 7
To support this method I built a framework composed by a library of templates with 
annotation tools and aggregation methods.

And a system represented in this diagram.

There are some very interesting points about this system, it allows you create and execute 
cascading microtasks processess to generate complex media annotation,
including different types os annotation tasks.

You can receive contribution of crowdsourcing platforms such as amazon mechanical turk,
closed groups at your lab, even open crowds recruited by facebook, for example.

hummm... at the same time.

Plus...using this architecture your dataset and the contributions from workers are save,
because evenwhen you use a crowdsourcing platforms, your data never is stored there.


I believe my time is over, but I can show you an example latter.

Thank you.




















































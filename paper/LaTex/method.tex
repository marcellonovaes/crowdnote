The method presented in this article aims to achieve a complex annotation of media objects such as audios, videos, images and texts. The differential of this method is that it allows us to reach these annotations using accessible resources such as simple annotation tools and the work of unskilled and untrained workers.

For this, a crowdsourcing approach was adopted, following a production model based on the cascade of microtasks, as can be seen in Figure~\ref{method}. In this way, the final result is built as the tasks are performed, with a task complementing and refining the result of the previous one.

This method follows three phases: Planning, Production, and Delivery. In the Planning phase, the process workflow is defined, in the Production phase the contributions are collected and processed and, in the Delivery phase, the outcome is available for consumption.

\subsection{Planning Phase} 
All activities involved in this step are performed by the crowdsourcer, who started the media annotation process. The objective of this phase is to determine what should be produced and how the production process should be.

Once the desired complex media annotation has been defined, it is analyzed and all information required for its construction is identified. Then a plan is defined for the construction of the outcome, determining the order in which the partial results must be produced, refined and improved. Each partial result corresponds to the output from the aggregation activity related to one of the tasks.

Therefore, it is necessary to design the necessary microtask to obtain each partial result and the order of dependency between them. For each of these tasks, it is necessary to define:
\begin{itemize}
\item What kinds of point of interest should be annotated.\\e.g. events, objects, subjects, issues.
\item What annotation type will be used for each of these kinds.\\e.g. free write, item selection, button click, image upload.
\item What data type will be collected for each annotation type.\\e.g. plain text, location, image, video.
\end{itemize}

Once the microtasks are defined, a workflow can be constructed to determine how they fit into the complex annotation production process. The built workflow should follow the general model shown in Figure ~\ref{method}. This workflow will guide the production phase.

Another important point to be defined in the planning phase is how the jobs should be distributed among the workers in each task. This concerns the distribution method and the stopping criteria. For example, in a given project, can be prioritized the annotation of items that have not yet received contributions, and set a maximum number of contributions per item as stop criteria.

Also at this stage should be defined the item that should be sent to the worker for each job. In this way, it is necessary to define the segmentation or selection strategy of the media objects to be annotated. In the case of image data sets, it is necessary to determine the set to be sent in each job, and if it is audio and video, determine the segmentation strategy. Segmentation can be done, for example, by duration (ex: send a 5 seconds segment to each worker), or using contextual criteria such as to send to each user a segment that contains a single dialog.

Finally, the annotation tools that will be used to collect the contributions in each of the microtasks are chosen.


\subsection{Production Phase}
In the production phase, the workflow (Figure~\ref{method}) defined in the planning phase must be executed. This workflow illustrates a cascade of sub-processes where the output of one of them is used as input to the next. Each of these sub-processes, which is represents to a microtask, is composed of four activities: (i) select workers, (ii) distribute task, (iii) task, and (iv) aggregation.

The proposed method recommends using a own system to perform the tasks of this phase, using from the crowdsourcing platforms only the workforce of your crowd. However, you can follow the method using features provided by crowdsourcing commercial environments such as Microworkers, CrowdFlower, and Amazon Mechanical Turk.

The system developed for the experiment carried out offers all the resources to carry out the production phase. This system can be obtained, used and extended freely \footnote{https://github.com/[REMOVED FOR BLIND REVIEW]}. It support the following activities of the production phase:

\begin{enumerate}
\item  \textbf{Select Workers:} recruitment and selection of workers can be done in different ways. When using a crowdsourcing platform, this activity is outsourced to it simply by defining the desired profile and how much will be paid for work. In this method, this activity can also be done by an open call, or by using a closed group of selected workers. For validation experiments, it is still possible to use a group of experts.

\item  \textbf{Distribute Task:} the distribution of tasks can be done by both the crowdsourcing platform and a own system, even when using an commercial crowdsourcing platform. To ensure better control of the distribution process as well as the stop criteria, the presented method recommends that a own system be used to distribute tasks even when you use the crowdsourcing platform. This system is responsible for determining the information about the next job to be delegated to a worker, such as the item to be noted.

\item  \textbf{Performe the Task:} the execution of the microtask consists of to present for the worker the job received by the distribution process and collecting its contribution. Each task must be presented to the worker through the correct annotation tool, giving him the necessary instructions so he can perform the job. %The method recommends that contributions be sent and stored on their own database so that they are always accessible for aggregation methods and for partial consultations. %However, if this is not possible storing the contributions provided by the crowdsourcing platform and retrieving them at the end of each sub-process.

\item  \textbf{Realize the Aggregation:} the aggregation is responsible for verify, filter, group, and process the collected annotations of the crowd according to the rules defined for each task. In this approach, the aggregation process can be fully automatic, supervised or manual. Manual aggregation is useful when is desired to evaluate each contribution as in authoring tasks. Supervise aggregation can be a good option when is possible to apply automatic methods but is required human verification of the result. Automatic aggregation is the default choice, this class of methods includes grouping, comparing, counting, calculating and other operations. The aggregation activity can generate different outputs, which can feed the next task and generate other artifacts. For example, the output of a microtask in which workers annotate events occurring in a video may, in addition to fueling a next task, generate indexes and summaries for this video.

\end{enumerate}

%The workflow produced in the preparation step defines how the output from a task will be used as input to the next one. In this way, the output of the last task is equal to the desired outcome of the process. In addition, each task can generate multiple outputs that are different representations of the collected and aggregated data. For example, events identified in a video by a task can either serve as input to the next task or generate a event-based index for the video.

%This cascading microtasks workflow is illustrated in Figure~\ref{method}. 
%It is important to notice that each iteration in this workflow is composed of two activities, the task in self and the aggregation activity, that generates the output from the obtained contributions. 


%The method determines that annotations are collected from the workers on each of the tasks, however, these annotations can be obtained from crowds or crowdsourcing indoor environments, or from open crowds brought together by an open call.


\subsection{Delivery Phase} 
The result generated by the aggregation activity of the last performed task in the workflow is the outcome of the production process. The delivery phase is responsible for make this outcome available to consumers, displayed by a player or exported in different formats because the complex annotation generated is stored as metadata and offers flexibility in this regard.












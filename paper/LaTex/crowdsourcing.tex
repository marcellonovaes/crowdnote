Human computation approaches can improve performance by division of labor because it helps to  tasks that can be executed in parallel. Each worker performs their work independently, so that the instances of a task can be executed in parallel, according to the Human Computation paradigm \cite{Rohwer:2010:NHC:1837885.1837897}.

To support this kind of cooperative process, crowdsourcing has emerged as a proposal to annotate media objects using a large number of contributors efficiently \cite{VonAhn:2005:HC:1168246}. Following the crowdsourcing principles, the tasks distributed to the workers are modeled to be done independently, maximizing the parallelism \citep{Howe2006}. Moreover, each task can be sent to many contributors, making possible to compare, check and to aggregate the contributions also reducing the chance of producing a biased result \cite{GALTON1907}.

This approach generally delivers good quality results using contributions from a crowd of contributors and can distribute, collect, validate and combine large amounts of contributions \cite{Hong:2011:GCR:2018966.2018970,Haas:2015:AMC:2824032.2824062,Mo:2013:OPH:2505515.2505755}. Since this approach is designed to handle a huge number of collaborators and contributions for tasks that require human intelligence \cite{Howe2006}, Crowdsourcing is appropriate to allow the Human Computing paradigm to be applied in a massive-scale online collaboration \cite{TEDMassive}.
% scenario by increasing the performance of the system paralleling tasks \cite{Rohwer:2010:NHC:1837885.1837897}, and improving accuracy according to the Wisdom of Crowd concept.

Crowdsourcing  is supported by four pillars:  The Crowdsourced Task, The Crowdsourcer, The Crowd, and The Crowdsourcing Plataform \cite{6861072}.

\begin{itemize}

\item{\textbf{The Crowdsourced Task}} is the HIT designed, according to the Human Computing paradigm, to acquire workers' contributions. Instances are generated of the task that are presented to the workers as jobs that must be performed \cite{Difallah:2015:DMC:2736277.2741685}.

\item{\textbf{The Crowdsourcer}} is the owner of a project, it may be an individual or institution that wishes to have a completed task. The owner is responsible for starting the crowdsourcing process, defining what task must be completed and how it should be presented to the workers as jobs \cite{6861072}.

\item{\textbf{The Crowd}} is the work force that moves the process once it is composed of all the workers who perform the jobs needed to generate the outcome. Each worker carries out his work independently, so that the works can be executed in parallel, according to the paradigm of Human Computation \cite{Rohwer:2010:NHC:1837885.1837897}.

\item{\textbf{The Crowdsourcing Platform}} is the centralizer of the whole proces, serves as an entry point for both the owner making the tasks available and for the workers to execute them. This kind of environment can be something sophisticated such as CrowdFlower, Microworkers, and AmazonMechanical Turk \cite{Difallah:2015:DMC:2736277.2741685}, or really simple systems with screens and forms for data collection such the mobile application used by the Google Crowdsource project \cite{google_cs}. A crowdsourcing environment is necessary, as the tasks must be made available to a potentially large number of workers. The crowdsourcing platform is a key element of support to massive-scale collaboration.

\end{itemize}

The use of a commercial crowdsourcing platform brings benefits such as not having to worry about management's issues of workers, jobs and contributions such as employee recruitment and collection of contributions as well as facilitating employee payments. Also, payouts are a good way to motivate and keep the crowd, although there are other coping factors such as personal accomplishment and even methods of gambling.

To exemplify how Crowdsourcing adds support for massive-scale online collaborations to human computer systems, one can analyze the collaborative processes involved in Luis von Ahn's three most well-known projects,  reCAPTCHA \cite{Simmons:2010:PLV:1869086.1869102},  ESP Game \cite{Robertson:2009:REG:1520340.1520597}, and  Duolingo \cite{vonAhn:2011:THC}.

ESP Game used a game as an input interface, so players who score points by adding tags that describe the images presented to them are viewed by the platform as workers annotating a base of images \cite{Robertson:2009:REG:1520340.1520597}. CAPTCHA tests are very popular on the internet, being widely used by various applications and sites, reCAPTCHA collects the responses that users provide for these tests, and so the platform also sees these interactions as workers annotating images reCAPTCHA \cite{Simmons:2010:PLV:1869086.1869102}. In the Duolingo users carry out translations while learning and practicing another language, and the platform collects these translations as contributions from the workers \cite{Abaunza:2016:BRW:3012430.3012522}.

It is possible to observe in these examples, that in addition the modeling according to the Human Computation paradigm, present a viable way of distributing jobs and collecting results for a potentially huge crowd of workers so that these contributions can be used to complete a task.



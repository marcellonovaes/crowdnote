%Crowdsourcing complex creative tasks remains difficult, in part because these tasks require skilled workers \cite{Dontcheva:2014:CCL:2556288.2557217}. 

Media annotation consists of supplementing media objects, such as videos, images, and audios, by adding metadata about their content and context, it is also used for describing media characteristics such as quality, encoding, among other features \cite{Wang:2009:BDM:1652990.1653002}. This supplementary information can be used to make easier the work of users and systems that can handle annotated items \cite{172450}. 

\pagebreak

Annotations can be used to highlight key points and add information to contents presented \cite{Cunha:2015:MVA:2820426.2820449}, facilitating the creation of media applications for content-based distribution \cite{Zhang:2012:KIE:2339530.2339620}, indexing \cite{Zhang:2007:PRS:1290082.1290126}, summarization \cite{Fiao:2016:AGS:3001773.3001802}, navigation \cite{Goldman:2008}, composition \cite{Wilk:2015:VCC:2713168.2713178} and more, through automatic and manual means \cite{Wang:2011:ALM:1899412.1899414,Mihalcea:2007:WLD:1321440.1321475}. 

Considering the amount of information, the number of interactions, and the expertise needed to generate an annotation, it is classified in this paper as simple and complex. While simple annotations can be acquired with a simple interaction of workers in a microtask, a complex annotation requires the worker to perform a more tedious, difficult, or time-consuming task in which he needs to perform multiple interactions.

Automatic approaches, such as rule-based and deep learning, usually present satisfactory results in generating media annotation, although they require well-structured media objects and extensive examples database \cite{lecun2015deep}. Thus, spontaneous scenarios involving unplanned and non-standard videos, images, and audios may not provide the requirements to apply these automatic techniques for media annotation \cite{murthy2015automatic}. 

Manual media annotation is suitable for these scenarios because it uses human intelligence to handle the tasks. However, this approach can be high-costly because of the potentially high-density of annotation points in the time-based media, as well as the complex nature of some annotation tasks.

Distributed approaches, whether cooperative or collaborative, are an alternative to bypass the high effort required for manual annotation. In a collaborative approach the contributors work together to solve the main problem. In a cooperative approach each contributor solves a part of the main problem to produce a final result  \cite{misanchuk2001building}.

Crowdsourcing allows the execution of a large-scale cooperative process for media annotations, using a large number of contributors efficiently. \cite{VonAhn:2005:HC:1168246}. Following the crowdsourcing principles, the tasks distributed to the workers are modeled to be done independently, maximizing the parallelism \citep{Howe2006}. Moreover, each task can be sent to many contributors, allowing to compare, check and aggregate the contributions, reducing the chance of producing a biased result \cite{GALTON1907}.

There are frequent problems in using a crowdsourcing approach to media annotation such as balancing the relationship between the complexity of the task and the cost. This cost refers to the profile and qualification required from the workers as to the complexity and difficulty of the task delegated to them.

Complex annotation usually requires more complex tasks, demanding some expertise from contributors and are harder and time-consuming to them. Otherwise, simple annotation tasks can be done by microtasks that can be performed easily and quickly by less skilled workers \cite{Difallah:2015:DMC:2736277.2741685}. 

Thus, as the goal is to use untrained and unskilled workers, the process must be modeled based on simple annotation microtasks.





The method presented in this paper aims to achieve a complex media annotation without requiring trained workers or experts, by employing a set of simple annotation tools rather than complex and expensive annotation systems. In this way, a complex annotation process is divided into a set of simple annotation microtasks, and based on them is defined a workflow for generating the outcome. This approach presents some characteristics to get around some problems faced in achieving a complex media annotation, including:


\begin{itemize}

\item Manual annotation is used to dispense examples bases and restricted conditions.

\item The annotations are provided by ordinary contributors rather than experts or trained workers.

\item It is based on simple and quick microtasks instead of hard, time-consuming and tedious tasks.

\item Simple annotation tools are used instead of complex systems.

\end{itemize}



Following this method, each simple annotation microtask is modeled here as a process composed of two steps: Collection and Aggregation. In the Collection step the contributions are received from the crowd, and in the Aggregation step these contributions are processed in order to generate its output.

%Also, each microtask is treated as a human computation function, producing an output that can be used as input to the next one in the workflow. In this way, the complex annotation production workflow is treated as a human computation algorithm. This point of view allows designing some features such as generate multiple outputs from a microtask to create simple outcomes from each partial result. These outcomes can be datasets, summaries, marks and more, so a complex annotation process can generate multiple output artifacts.


This method is an updated version of [REMOVED FOR BLIND REVIEW]\citep{172450}, with improvements that made it more flexible and useful by overcoming some important limitations detected in the previous version. In addition, improvements were made to the framework designed to support the method. The main improvements consist in:
\begin{itemize}

\item The method scope has been expanded to any media object, not just videos.

\item Support for parallel and serial tasks.

\item Support for non-linear process workflows.

\item Improved diagrams for the method.

\item Improved diagrams for the framework architecture.

\item Support to multiply outputs from each microtask.

\item Support for automatic, manual and supervised aggregation methods, instead of just automatic ones.

\item Aggregation methods have incorporated more features.

\item The annotation tools was refined.

\item In the experiment was used the commercial crowdsourcing platform Microworkers, instead of an open crowd.

\end{itemize}

The objective of this work is to present the updated version of the method and verify if it is possible to design and execute a complex media annotation process from it and the improved framework. For this, a video enrichment system was developed, based on structure. This system was used to perform an experiment in which the crowd was responsible for performing the tasks related to the enrichment process. The result of this experiment was based on the base provided by the original video's author according to his expectations for the enriched version of the video.


The rest of this paper is structured as follows. Section 2 presents the concepts used and how they are employed in the proposed approach. Section 3 presents related works. Section 4 presents the method. Section 5 presents the framework architecture. Section 6 presents the conducted experiment. Finally, section 7 concludes the paper presenting final considerations and future prospects.